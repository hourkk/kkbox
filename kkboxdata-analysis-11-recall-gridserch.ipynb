{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊跟前面比較 利用改變訓練資料 使得流失跟非流失比率調整1:1 強化對流失的判斷 看看結果是否增加\n",
    "\n",
    "最後選擇高分模型再做參數挑整 找出更適合模型\n",
    "\n",
    "再由最後的測試資料驗證我們的模型是否跟訓練時目標準確度一樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "      <th>('num_25', 'mean')</th>\n",
       "      <th>('num_50', 'mean')</th>\n",
       "      <th>('num_75', 'mean')</th>\n",
       "      <th>...</th>\n",
       "      <th>('total_secs', 'mean')</th>\n",
       "      <th>play_days</th>\n",
       "      <th>payment_method_id_mode</th>\n",
       "      <th>payment_plan_days_mode</th>\n",
       "      <th>plan_list_price_sum</th>\n",
       "      <th>actual_amount_paid_sum</th>\n",
       "      <th>is_auto_renew_sum</th>\n",
       "      <th>is_cancel_sum</th>\n",
       "      <th>transactions_times</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>+tJonkh+O1CA796Fm5X60UMOtB6POHAwPjbTRVl/EuU=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2011-09-14</td>\n",
       "      <td>5.258621</td>\n",
       "      <td>1.017241</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>...</td>\n",
       "      <td>7163.224276</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I0yFvqMoNkM8ZNHb617e1RBzIS/YRKemHO7Wj13EtA0=</td>\n",
       "      <td>13</td>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>2011-09-18</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>...</td>\n",
       "      <td>4270.220462</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>OoDwiKZM+ZGr9P3fRivavgOtglTEaNfWJO4KaJcTTts=</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2011-09-18</td>\n",
       "      <td>1.523810</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>3052.110524</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4De1jAxNRABoyRBDZ82U0yEmzYkqeOugRGVNIf92Xb8=</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>2011-09-20</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>3557.196400</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Z6WIOK9vXy+e2XDBiioNAxuZ0ScXSU/Ebq4tUwqVSrE=</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>female</td>\n",
       "      <td>9</td>\n",
       "      <td>2011-09-29</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1662.418200</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          msno  city  bd  gender  \\\n",
       "0           0  +tJonkh+O1CA796Fm5X60UMOtB6POHAwPjbTRVl/EuU=     1   0     NaN   \n",
       "1           1  I0yFvqMoNkM8ZNHb617e1RBzIS/YRKemHO7Wj13EtA0=    13  63    male   \n",
       "2           2  OoDwiKZM+ZGr9P3fRivavgOtglTEaNfWJO4KaJcTTts=     1   0     NaN   \n",
       "3           3  4De1jAxNRABoyRBDZ82U0yEmzYkqeOugRGVNIf92Xb8=     4  28  female   \n",
       "4           4  Z6WIOK9vXy+e2XDBiioNAxuZ0ScXSU/Ebq4tUwqVSrE=    22  38  female   \n",
       "\n",
       "   registered_via registration_init_time  ('num_25', 'mean')  \\\n",
       "0               7             2011-09-14            5.258621   \n",
       "1               9             2011-09-18            1.384615   \n",
       "2               7             2011-09-18            1.523810   \n",
       "3               9             2011-09-20            2.200000   \n",
       "4               9             2011-09-29            1.800000   \n",
       "\n",
       "   ('num_50', 'mean')  ('num_75', 'mean')    ...     ('total_secs', 'mean')  \\\n",
       "0            1.017241            0.827586    ...                7163.224276   \n",
       "1            0.576923            0.538462    ...                4270.220462   \n",
       "2            0.285714            0.285714    ...                3052.110524   \n",
       "3            0.400000            0.200000    ...                3557.196400   \n",
       "4            1.000000            0.700000    ...                1662.418200   \n",
       "\n",
       "   play_days  payment_method_id_mode  payment_plan_days_mode  \\\n",
       "0         58                      41                      30   \n",
       "1         26                      40                      30   \n",
       "2         21                      41                      30   \n",
       "3          5                      36                      30   \n",
       "4         20                      40                      30   \n",
       "\n",
       "   plan_list_price_sum  actual_amount_paid_sum  is_auto_renew_sum  \\\n",
       "0                  258                     258                  2   \n",
       "1                  298                     298                  2   \n",
       "2                  298                     298                  2   \n",
       "3                  360                     360                  2   \n",
       "4                  298                     298                  2   \n",
       "\n",
       "   is_cancel_sum  transactions_times  is_churn  \n",
       "0              0                   2         0  \n",
       "1              0                   2         0  \n",
       "2              0                   2         0  \n",
       "3              0                   2         0  \n",
       "4              0                   2         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkboxdata2017=pd.read_csv('kkboxdata2017v2.csv',parse_dates=['registration_init_time'])\n",
    "kkboxdata2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 770929 entries, 0 to 770928\n",
      "Data columns (total 23 columns):\n",
      "Unnamed: 0                770929 non-null int64\n",
      "msno                      770929 non-null object\n",
      "city                      770929 non-null int64\n",
      "bd                        770929 non-null int64\n",
      "gender                    356099 non-null object\n",
      "registered_via            770929 non-null int64\n",
      "registration_init_time    770929 non-null datetime64[ns]\n",
      "('num_25', 'mean')        770929 non-null float64\n",
      "('num_50', 'mean')        770929 non-null float64\n",
      "('num_75', 'mean')        770929 non-null float64\n",
      "('num_985', 'mean')       770929 non-null float64\n",
      "('num_100', 'mean')       770929 non-null float64\n",
      "('num_unq', 'mean')       770929 non-null float64\n",
      "('total_secs', 'mean')    770929 non-null float64\n",
      "play_days                 770929 non-null int64\n",
      "payment_method_id_mode    770929 non-null int64\n",
      "payment_plan_days_mode    770929 non-null int64\n",
      "plan_list_price_sum       770929 non-null int64\n",
      "actual_amount_paid_sum    770929 non-null int64\n",
      "is_auto_renew_sum         770929 non-null int64\n",
      "is_cancel_sum             770929 non-null int64\n",
      "transactions_times        770929 non-null int64\n",
      "is_churn                  770929 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(7), int64(13), object(2)\n",
      "memory usage: 135.3+ MB\n"
     ]
    }
   ],
   "source": [
    "kkboxdata2017.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為類別太多 先做虛擬變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'msno', 'bd', 'registration_init_time',\n",
       "       '('num_25', 'mean')', '('num_50', 'mean')', '('num_75', 'mean')',\n",
       "       '('num_985', 'mean')', '('num_100', 'mean')', '('num_unq', 'mean')',\n",
       "       '('total_secs', 'mean')', 'play_days', 'payment_plan_days_mode',\n",
       "       'plan_list_price_sum', 'actual_amount_paid_sum', 'is_auto_renew_sum',\n",
       "       'is_cancel_sum', 'transactions_times', 'city__3.0', 'city__4.0',\n",
       "       'city__5.0', 'city__6.0', 'city__7.0', 'city__8.0', 'city__9.0',\n",
       "       'city__10.0', 'city__11.0', 'city__12.0', 'city__13.0', 'city__14.0',\n",
       "       'city__15.0', 'city__16.0', 'city__17.0', 'city__18.0', 'city__19.0',\n",
       "       'city__20.0', 'city__21.0', 'city__22.0', 'city__nan', 'gender__male',\n",
       "       'gender__nan', 'registered_via__4.0', 'registered_via__7.0',\n",
       "       'registered_via__9.0', 'registered_via__13.0', 'registered_via__nan',\n",
       "       'payment_method_id_mode__11.0', 'payment_method_id_mode__14.0',\n",
       "       'payment_method_id_mode__16.0', 'payment_method_id_mode__17.0',\n",
       "       'payment_method_id_mode__18.0', 'payment_method_id_mode__19.0',\n",
       "       'payment_method_id_mode__21.0', 'payment_method_id_mode__23.0',\n",
       "       'payment_method_id_mode__26.0', 'payment_method_id_mode__27.0',\n",
       "       'payment_method_id_mode__28.0', 'payment_method_id_mode__29.0',\n",
       "       'payment_method_id_mode__30.0', 'payment_method_id_mode__31.0',\n",
       "       'payment_method_id_mode__32.0', 'payment_method_id_mode__33.0',\n",
       "       'payment_method_id_mode__34.0', 'payment_method_id_mode__35.0',\n",
       "       'payment_method_id_mode__36.0', 'payment_method_id_mode__37.0',\n",
       "       'payment_method_id_mode__38.0', 'payment_method_id_mode__39.0',\n",
       "       'payment_method_id_mode__40.0', 'payment_method_id_mode__41.0',\n",
       "       'payment_method_id_mode__nan', 'is_churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummiesc=['city','gender','registered_via','payment_method_id_mode']\n",
    "kkboxdata2017 =pd.get_dummies(kkboxdata2017, prefix_sep=\"__\",columns=dummiesc,dummy_na=True,drop_first=True)\n",
    "cols = list(kkboxdata2017)\n",
    "cols.pop(cols.index('is_churn'))\n",
    "cols.append('is_churn')\n",
    "kkboxdata2017 = kkboxdata2017.loc[:,cols]\n",
    "kkboxdata2017.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料 設計在20161201之前註冊會員"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    703552\n",
       "1     36157\n",
       "Name: is_churn, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata=kkboxdata2017.query('registration_init_time<20161202')\n",
    "traindata['is_churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "驗證資料 設計在20161202與20170101之間註冊會員"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13920\n",
       "1     1016\n",
       "Name: is_churn, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validdata=kkboxdata2017.query('20170101>registration_init_time>20161201')\n",
    "validdata['is_churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試資料 設計在20170101之後註冊會員"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13971\n",
       "1     1857\n",
       "Name: is_churn, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata=kkboxdata2017.query('registration_init_time>20170101')\n",
    "testdata['is_churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看結果而言 流失比率是少數 如果要找出預測流失客群 屬於二元分類問題 另外要對資料不平衡的情況做處理\n",
    "\n",
    "\n",
    "先塞選好資料需要用的留"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刪除不用的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "traindata.drop(['Unnamed: 0', 'msno','registration_init_time',], axis=1,inplace =True)\n",
    "validdata.drop(['Unnamed: 0', 'msno','registration_init_time',], axis=1,inplace =True)\n",
    "testdata.drop(['Unnamed: 0', 'msno','registration_init_time',], axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料流失跟非流失各取30000做1:1訓練 再由正常的驗證資料 看看變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bd</th>\n",
       "      <th>('num_25', 'mean')</th>\n",
       "      <th>('num_50', 'mean')</th>\n",
       "      <th>('num_75', 'mean')</th>\n",
       "      <th>('num_985', 'mean')</th>\n",
       "      <th>('num_100', 'mean')</th>\n",
       "      <th>('num_unq', 'mean')</th>\n",
       "      <th>('total_secs', 'mean')</th>\n",
       "      <th>play_days</th>\n",
       "      <th>payment_plan_days_mode</th>\n",
       "      <th>...</th>\n",
       "      <th>payment_method_id_mode__34.0</th>\n",
       "      <th>payment_method_id_mode__35.0</th>\n",
       "      <th>payment_method_id_mode__36.0</th>\n",
       "      <th>payment_method_id_mode__37.0</th>\n",
       "      <th>payment_method_id_mode__38.0</th>\n",
       "      <th>payment_method_id_mode__39.0</th>\n",
       "      <th>payment_method_id_mode__40.0</th>\n",
       "      <th>payment_method_id_mode__41.0</th>\n",
       "      <th>payment_method_id_mode__nan</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159857</th>\n",
       "      <td>26</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>12542.559600</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310481</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>216.573000</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411213</th>\n",
       "      <td>33</td>\n",
       "      <td>2.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>14.526316</td>\n",
       "      <td>12.710526</td>\n",
       "      <td>3638.219421</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752539</th>\n",
       "      <td>0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>8.285714</td>\n",
       "      <td>966.461571</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384406</th>\n",
       "      <td>0</td>\n",
       "      <td>1.705882</td>\n",
       "      <td>1.058824</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>4.764706</td>\n",
       "      <td>6.647059</td>\n",
       "      <td>1490.513118</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        bd  ('num_25', 'mean')  ('num_50', 'mean')  ('num_75', 'mean')  \\\n",
       "159857  26            6.600000            3.000000            3.400000   \n",
       "310481   0            0.000000            0.000000            0.000000   \n",
       "411213  33            2.473684            0.473684            0.342105   \n",
       "752539   0            2.857143            1.857143            1.000000   \n",
       "384406   0            1.705882            1.058824            0.352941   \n",
       "\n",
       "        ('num_985', 'mean')  ('num_100', 'mean')  ('num_unq', 'mean')  \\\n",
       "159857             2.800000            49.600000            14.600000   \n",
       "310481             1.000000             0.000000             1.000000   \n",
       "411213             0.236842            14.526316            12.710526   \n",
       "752539             0.714286             2.571429             8.285714   \n",
       "384406             0.588235             4.764706             6.647059   \n",
       "\n",
       "        ('total_secs', 'mean')  play_days  payment_plan_days_mode    ...     \\\n",
       "159857            12542.559600          5                      30    ...      \n",
       "310481              216.573000          1                      30    ...      \n",
       "411213             3638.219421         38                      30    ...      \n",
       "752539              966.461571          7                      30    ...      \n",
       "384406             1490.513118         17                      30    ...      \n",
       "\n",
       "        payment_method_id_mode__34.0  payment_method_id_mode__35.0  \\\n",
       "159857                             0                             0   \n",
       "310481                             0                             0   \n",
       "411213                             0                             0   \n",
       "752539                             0                             0   \n",
       "384406                             0                             0   \n",
       "\n",
       "        payment_method_id_mode__36.0  payment_method_id_mode__37.0  \\\n",
       "159857                             0                             0   \n",
       "310481                             0                             0   \n",
       "411213                             0                             0   \n",
       "752539                             0                             0   \n",
       "384406                             1                             0   \n",
       "\n",
       "        payment_method_id_mode__38.0  payment_method_id_mode__39.0  \\\n",
       "159857                             0                             0   \n",
       "310481                             0                             0   \n",
       "411213                             0                             0   \n",
       "752539                             1                             0   \n",
       "384406                             0                             0   \n",
       "\n",
       "        payment_method_id_mode__40.0  payment_method_id_mode__41.0  \\\n",
       "159857                             0                             1   \n",
       "310481                             0                             1   \n",
       "411213                             0                             1   \n",
       "752539                             0                             0   \n",
       "384406                             0                             0   \n",
       "\n",
       "        payment_method_id_mode__nan  is_churn  \n",
       "159857                            0         0  \n",
       "310481                            0         0  \n",
       "411213                            0         0  \n",
       "752539                            0         1  \n",
       "384406                            0         1  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata0 = pd.concat([traindata.query('is_churn==0').sample(30000),traindata.query('is_churn==1').sample(30000)])\n",
    "from sklearn.utils import shuffle\n",
    "traindata0 = shuffle(traindata0)\n",
    "traindata0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類別要使用虛擬變數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊要做分類的機器學習 這邊我先把主流的分類器跑一次 之後再挑選幾個作參數調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    #\"rbf SVM\": SVC(kernel='rbf',probability=True),\n",
    "    #\"linear SVM\": SVC(kernel='linear',probability=True),\n",
    "    #\"poly SVM\": SVC(kernel='poly',probability=True),\n",
    "    #\"sigmoid SVM\": SVC(kernel='sigmoid',probability=True),\n",
    "    \"linear SVM sgd\": SGDClassifier(loss='hinge'),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=100),\n",
    "    \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"Neural Net\": MLPClassifier(alpha = 1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"AdaBoost\" : AdaBoostClassifier()\n",
    "    #\"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    #\"Gaussian Process\": GaussianProcessClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 5, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "    is because it is very easy to save the whole dictionary with the pickle module.\n",
    "    \n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n",
    "        t_start = time.perf_counter()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.perf_counter()\n",
    "        \n",
    "        t_diff = t_end - t_start\n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        test_recall=recall_score(Y_test,classifier.predict(X_test))\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'recall': test_recall, 'train_time': t_diff}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models\n",
    "\n",
    "\n",
    "\n",
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_r = [dict_models[key]['recall'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),5)), columns = ['classifier', 'train_score', 'test_score', 'recall', 'train_time'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'recall'] = training_r[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來利用 隨機森林幫助我們先挑選可能比較有用的變數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) is_cancel_sum                  0.174265\n",
      " 2) is_auto_renew_sum              0.089702\n",
      " 3) play_days                      0.073530\n",
      " 4) actual_amount_paid_sum         0.061961\n",
      " 5) plan_list_price_sum            0.059900\n",
      " 6) ('total_secs', 'mean')         0.048926\n",
      " 7) ('num_unq', 'mean')            0.048279\n",
      " 8) ('num_100', 'mean')            0.047850\n",
      " 9) ('num_25', 'mean')             0.047415\n",
      "10) transactions_times             0.046819\n",
      "11) ('num_50', 'mean')             0.046249\n",
      "12) ('num_985', 'mean')            0.046059\n",
      "13) ('num_75', 'mean')             0.045344\n",
      "14) payment_method_id_mode__38.0   0.032025\n",
      "15) bd                             0.029572\n",
      "16) payment_method_id_mode__39.0   0.012586\n",
      "17) registered_via__9.0            0.006128\n",
      "18) gender__male                   0.006017\n",
      "19) payment_method_id_mode__41.0   0.005805\n",
      "20) registered_via__4.0            0.005448\n",
      "21) payment_method_id_mode__29.0   0.005264\n",
      "22) registered_via__7.0            0.005039\n",
      "23) payment_method_id_mode__36.0   0.004929\n",
      "24) city__13.0                     0.004651\n",
      "25) gender__nan                    0.004572\n",
      "26) city__5.0                      0.004309\n",
      "27) city__4.0                      0.003611\n",
      "28) city__15.0                     0.003443\n",
      "29) city__22.0                     0.003309\n",
      "30) payment_plan_days_mode         0.003199\n",
      "31) city__6.0                      0.002706\n",
      "32) city__14.0                     0.002049\n",
      "33) city__12.0                     0.001741\n",
      "34) payment_method_id_mode__40.0   0.001614\n",
      "35) city__11.0                     0.001430\n",
      "36) city__8.0                      0.001412\n",
      "37) city__9.0                      0.001352\n",
      "38) city__18.0                     0.001327\n",
      "39) city__10.0                     0.001119\n",
      "40) city__21.0                     0.001065\n",
      "41) city__17.0                     0.000945\n",
      "42) city__3.0                      0.000934\n",
      "43) payment_method_id_mode__28.0   0.000824\n",
      "44) payment_method_id_mode__17.0   0.000755\n",
      "45) payment_method_id_mode__30.0   0.000743\n",
      "46) payment_method_id_mode__37.0   0.000621\n",
      "47) city__7.0                      0.000591\n",
      "48) payment_method_id_mode__33.0   0.000483\n",
      "49) payment_method_id_mode__34.0   0.000417\n",
      "50) registered_via__13.0           0.000236\n",
      "51) city__20.0                     0.000199\n",
      "52) city__16.0                     0.000199\n",
      "53) payment_method_id_mode__31.0   0.000179\n",
      "54) payment_method_id_mode__27.0   0.000154\n",
      "55) payment_method_id_mode__35.0   0.000147\n",
      "56) payment_method_id_mode__26.0   0.000122\n",
      "57) payment_method_id_mode__16.0   0.000112\n",
      "58) payment_method_id_mode__21.0   0.000100\n",
      "59) payment_method_id_mode__18.0   0.000071\n",
      "60) payment_method_id_mode__23.0   0.000061\n",
      "61) city__19.0                     0.000040\n",
      "62) payment_method_id_mode__19.0   0.000038\n",
      "63) payment_method_id_mode__14.0   0.000007\n",
      "64) payment_method_id_mode__11.0   0.000000\n",
      "65) payment_method_id_mode__32.0   0.000000\n",
      "66) registered_via__nan            0.000000\n",
      "67) city__nan                      0.000000\n",
      "68) payment_method_id_mode__nan    0.000000\n"
     ]
    }
   ],
   "source": [
    "feat_labels = traindata.columns[:-1]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(traindata.iloc[:,:-1], traindata.iloc[:,-1])\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for f in range(traindata.shape[1]-1):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30,feat_labels[indices[f]],importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑選比重比較高的前21個再做訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Logistic Regression in 4.10 s\n",
      "trained Nearest Neighbors in 0.12 s\n",
      "trained linear SVM sgd in 0.08 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Gradient Boosting Classifier in 4.93 s\n",
      "trained Decision Tree in 0.67 s\n",
      "trained Random Forest in 9.21 s\n",
      "trained Neural Net in 4.54 s\n",
      "trained Naive Bayes in 0.04 s\n",
      "trained XGBoost in 3.28 s\n",
      "trained AdaBoost in 3.09 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.898099</td>\n",
       "      <td>0.958661</td>\n",
       "      <td>9.211591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.892167</td>\n",
       "      <td>0.896157</td>\n",
       "      <td>0.926181</td>\n",
       "      <td>4.544015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.929417</td>\n",
       "      <td>0.894885</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>4.927943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.928450</td>\n",
       "      <td>0.894416</td>\n",
       "      <td>0.963583</td>\n",
       "      <td>3.282818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.925450</td>\n",
       "      <td>0.893613</td>\n",
       "      <td>0.961614</td>\n",
       "      <td>3.091793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891470</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.673076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.920967</td>\n",
       "      <td>0.878548</td>\n",
       "      <td>0.971457</td>\n",
       "      <td>4.101097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.887067</td>\n",
       "      <td>0.829339</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.036974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.780833</td>\n",
       "      <td>0.772630</td>\n",
       "      <td>0.761811</td>\n",
       "      <td>0.116582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear SVM sgd</td>\n",
       "      <td>0.523183</td>\n",
       "      <td>0.146157</td>\n",
       "      <td>0.969488</td>\n",
       "      <td>0.083293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score    recall  train_time\n",
       "5                 Random Forest     0.999983    0.898099  0.958661    9.211591\n",
       "6                    Neural Net     0.892167    0.896157  0.926181    4.544015\n",
       "3  Gradient Boosting Classifier     0.929417    0.894885  0.962598    4.927943\n",
       "8                       XGBoost     0.928450    0.894416  0.963583    3.282818\n",
       "9                      AdaBoost     0.925450    0.893613  0.961614    3.091793\n",
       "4                 Decision Tree     1.000000    0.891470  0.874016    0.673076\n",
       "0           Logistic Regression     0.920967    0.878548  0.971457    4.101097\n",
       "7                   Naive Bayes     0.887067    0.829339  0.962598    0.036974\n",
       "1             Nearest Neighbors     0.780833    0.772630  0.761811    0.116582\n",
       "2                linear SVM sgd     0.523183    0.146157  0.969488    0.083293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_models = batch_classify(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1], validdata.loc[:,feat_labels[indices[0:21]]], validdata.iloc[:,-1], no_classifiers = 12)\n",
    "display_dict_models(dict_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟資料未調整比 recall明顯提升很多 linear SVM recall過低 不是我們想要的 其他模型表現皆有提升"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "針對上面的理想模型 再調參數 希望調出數值最佳的模型\n",
    "\n",
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train recall\n",
      "0.9595666666666667\n",
      "best train params\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 200}\n",
      "valid recall\n",
      "0.96751968503937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "scorer = make_scorer(recall_score, pos_label=1)\n",
    "param_test1 = {\n",
    " 'criterion':['gini', 'entropy'],\n",
    " 'max_leaf_nodes': [2,5,10,100,150,200,500]\n",
    "}\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(),\n",
    "                  param_grid = param_test1,scoring=scorer,n_jobs=-1,iid=False, cv=2)\n",
    "\n",
    "gs = gs.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gs.best_score_)\n",
    "print('best train params')\n",
    "print(gs.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gs.predict(validdata.loc[:,feat_labels[indices[0:21]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rff=RandomForestClassifier(criterion= 'entropy', max_leaf_nodes= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train recall\n",
      "0.9584666666666667\n",
      "best train params\n",
      "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}\n",
      "valid recall\n",
      "0.9635826771653543\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = dict(learning_rate=[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],max_depth= [1,2,3],n_estimators=[100,150])\n",
    "gs = GridSearchCV(estimator=GradientBoostingClassifier(),\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=scorer,\n",
    "                  cv=2,\n",
    "                  n_jobs = -1)\n",
    "gs = gs.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gs.best_score_)\n",
    "print('best train params')\n",
    "print(gs.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gs.predict(validdata.loc[:,feat_labels[indices[0:21]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbf=GradientBoostingClassifier({'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10),\n",
    " 'min_child_weight':range(1,11)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(         learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "min_child_weight=1, gamma=0, subsample=0.8,             colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',  scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1,     scoring=scorer,n_jobs=-1,iid=False, cv=2)\n",
    "gsearch1=gsearch1.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch1.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch1.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch1.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring=scorer,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch3=gsearch3.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch3.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch3.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch3.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring=scorer,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4=gsearch4.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch4.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch4.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch4.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "param_test7 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test7, scoring=scorer,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4=gsearch4.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch4.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch4.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch4.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "param_test7 = {\n",
    " 'reg_alpha':np.arange(0.01,0.05,0.01)\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test7, scoring=scorer,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch4=gsearch4.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch4.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch4.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch4.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "param_test8 = {\n",
    " 'learning_rate':[0.001,0.005,0.01,0.05,0.1,0.5]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test8, scoring=scorer,n_jobs=-1,iid=False, cv=2)\n",
    "gsearch5=gsearch5.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearch5.best_score_)\n",
    "print('best train params')\n",
    "print(gsearch5.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearch5.predict(validdata.loc[:,feat_labels[indices[0:21]]])))\n",
    "XGBb=GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,reg_alpha=0.01,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27)              \n",
    "gsearchb=gsearchb.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best train recall\n",
      "0.9611666666666667\n",
      "best train params\n",
      "{'learning_rate': 0.01, 'max_depth': 8, 'min_child_weight': 1, 'reg_alpha': 0.04}\n",
      "valid recall\n",
      "0.9724409448818898\n",
      "Wall time: 22min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_testa = {\n",
    " 'max_depth':range(3,10),\n",
    " 'min_child_weight':range(1,11),\n",
    " 'reg_alpha':np.arange(0.01,0.05,0.01),\n",
    " 'learning_rate':np.arange(0.01,0.05,0.01),\n",
    "}\n",
    "gsearchb = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27), \n",
    " param_grid = param_testa, scoring=scorer,n_jobs=-1,iid=False, cv=2)\n",
    "gsearchb=gsearchb.fit(traindata0.loc[:,feat_labels[indices[0:21]]], traindata0.iloc[:,-1])\n",
    "print('best train recall')\n",
    "print(gsearchb.best_score_)\n",
    "print('best train params')\n",
    "print(gsearchb.best_params_)\n",
    "print('valid recall')\n",
    "print(recall_score(validdata.iloc[:,-1],gsearchb.predict(validdata.loc[:,feat_labels[indices[0:21]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    " xgbf=XGBClassifier(learning_rate =0.01, n_estimators=177, max_depth=8,\n",
    " min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.6,reg_alpha=0.04,\n",
    " objective= 'binary:logistic', scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊完成我們理想模型 參數要調更細更準的話必須花費很多時間也不能保證測試可以相對提高只會有些微差距\n",
    "\n",
    "最後再拿最新的資料測試 做最後驗證 這邊假設是我們用舊的資料想要預測新的資料是否跟我們模型想的一樣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先取測試時間之前的檔案再抽樣做成訓練資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Logistic Regression in 5.12 s\n",
      "trained Nearest Neighbors in 0.12 s\n",
      "trained linear SVM sgd in 0.06 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Gradient Boosting Classifier in 4.71 s\n",
      "trained Decision Tree in 0.68 s\n",
      "trained Random Forest in 9.04 s\n",
      "trained Neural Net in 4.97 s\n",
      "trained Naive Bayes in 0.04 s\n",
      "trained XGBoost in 3.20 s\n",
      "trained AdaBoost in 2.99 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.885203</td>\n",
       "      <td>0.967690</td>\n",
       "      <td>9.039227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.923933</td>\n",
       "      <td>0.875284</td>\n",
       "      <td>0.980075</td>\n",
       "      <td>2.989527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.928050</td>\n",
       "      <td>0.874273</td>\n",
       "      <td>0.977383</td>\n",
       "      <td>4.705279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.927067</td>\n",
       "      <td>0.874084</td>\n",
       "      <td>0.980075</td>\n",
       "      <td>3.196426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873642</td>\n",
       "      <td>0.901454</td>\n",
       "      <td>0.678212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear SVM sgd</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.871367</td>\n",
       "      <td>0.140549</td>\n",
       "      <td>0.064770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.918467</td>\n",
       "      <td>0.863912</td>\n",
       "      <td>0.986537</td>\n",
       "      <td>5.116437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.884683</td>\n",
       "      <td>0.810020</td>\n",
       "      <td>0.976306</td>\n",
       "      <td>0.036625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.783583</td>\n",
       "      <td>0.753285</td>\n",
       "      <td>0.853527</td>\n",
       "      <td>0.119116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.762600</td>\n",
       "      <td>0.591168</td>\n",
       "      <td>0.996230</td>\n",
       "      <td>4.969629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score    recall  train_time\n",
       "5                 Random Forest     0.999967    0.885203  0.967690    9.039227\n",
       "9                      AdaBoost     0.923933    0.875284  0.980075    2.989527\n",
       "3  Gradient Boosting Classifier     0.928050    0.874273  0.977383    4.705279\n",
       "8                       XGBoost     0.927067    0.874084  0.980075    3.196426\n",
       "4                 Decision Tree     1.000000    0.873642  0.901454    0.678212\n",
       "2                linear SVM sgd     0.521100    0.871367  0.140549    0.064770\n",
       "0           Logistic Regression     0.918467    0.863912  0.986537    5.116437\n",
       "7                   Naive Bayes     0.884683    0.810020  0.976306    0.036625\n",
       "1             Nearest Neighbors     0.783583    0.753285  0.853527    0.119116\n",
       "6                    Neural Net     0.762600    0.591168  0.996230    4.969629"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainf = pd.concat([traindata,validdata])\n",
    "trainf = pd.concat([trainf.query('is_churn==0').sample(30000),trainf.query('is_churn==1').sample(30000)])\n",
    "trainf = shuffle(trainf)\n",
    "dict_modelf = batch_classify(trainf.loc[:,feat_labels[indices[0:21]]], trainf.iloc[:,-1], testdata.loc[:,feat_labels[indices[0:21]]], testdata.iloc[:,-1], no_classifiers = 12)\n",
    "display_dict_models(dict_modelf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是未調參數的情況下 之前預測的前三模型(Random Forest,Gradient Boosting Classifier\t,XGBoost)結果表現差不多\n",
    "再測試分數不能太低的情況下 想要最高的recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來看調整過參數 再用新的測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_classifiersf = {\n",
    "    \"Gradient Boosting Classifier\": xgbf,\n",
    "    \"Random Forest\": rff,\n",
    "    \"XGBoost\": xgbf,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_classifyf(X_train, Y_train, X_test, Y_test, no_classifiers = 5, verbose = True):\n",
    "    \"\"\"\n",
    "    This method, takes as input the X, Y matrices of the Train and Test set.\n",
    "    And fits them on all of the Classifiers specified in the dict_classifier.\n",
    "    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n",
    "    is because it is very easy to save the whole dictionary with the pickle module.\n",
    "    \n",
    "    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n",
    "    So it is best to train them on a smaller dataset first and \n",
    "    decide whether you want to comment them out or not based on the test accuracy score.\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_models = {}\n",
    "    for classifier_name, classifier in list(dict_classifiersf.items())[:no_classifiers]:\n",
    "        t_start = time.perf_counter()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.perf_counter()\n",
    "        \n",
    "        t_diff = t_end - t_start\n",
    "        train_score = classifier.score(X_train, Y_train)\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "        test_recall=recall_score(Y_test,classifier.predict(X_test))\n",
    "        \n",
    "        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'recall': test_recall, 'train_time': t_diff}\n",
    "        if verbose:\n",
    "            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n",
    "    return dict_models\n",
    "\n",
    "\n",
    "\n",
    "def display_dict_modelsf(dict_models, sort_by='test_score'):\n",
    "    cls = [key for key in dict_models.keys()]\n",
    "    test_s = [dict_models[key]['test_score'] for key in cls]\n",
    "    training_s = [dict_models[key]['train_score'] for key in cls]\n",
    "    training_r = [dict_models[key]['recall'] for key in cls]\n",
    "    training_t = [dict_models[key]['train_time'] for key in cls]\n",
    "    \n",
    "    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),5)), columns = ['classifier', 'train_score', 'test_score', 'recall', 'train_time'])\n",
    "    for ii in range(0,len(cls)):\n",
    "        df_.loc[ii, 'classifier'] = cls[ii]\n",
    "        df_.loc[ii, 'train_score'] = training_s[ii]\n",
    "        df_.loc[ii, 'test_score'] = test_s[ii]\n",
    "        df_.loc[ii, 'recall'] = training_r[ii]\n",
    "        df_.loc[ii, 'train_time'] = training_t[ii]\n",
    "    \n",
    "    display(df_.sort_values(by=sort_by, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Gradient Boosting Classifier in 11.23 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained Random Forest in 0.77 s\n",
      "trained XGBoost in 11.48 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>0.877053</td>\n",
       "      <td>0.979537</td>\n",
       "      <td>11.229288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>0.877053</td>\n",
       "      <td>0.979537</td>\n",
       "      <td>11.479489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.929817</td>\n",
       "      <td>0.867766</td>\n",
       "      <td>0.983306</td>\n",
       "      <td>0.765590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  train_score  test_score    recall  train_time\n",
       "0  Gradient Boosting Classifier     0.933500    0.877053  0.979537   11.229288\n",
       "2                       XGBoost     0.933500    0.877053  0.979537   11.479489\n",
       "1                 Random Forest     0.929817    0.867766  0.983306    0.765590"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_modelf = batch_classifyf(trainf.loc[:,feat_labels[indices[0:21]]], trainf.iloc[:,-1], testdata.loc[:,feat_labels[indices[0:21]]], testdata.iloc[:,-1], no_classifiers = 5)\n",
    "display_dict_modelsf(dict_modelf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "調整後 recall 除了XGBoost 其他皆有提升一點 推測XGBoost調整過多參數會有overfit情形 不過結果依然優秀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊結論是由舊的會員資料做模型預測新的資料 給我們想要的目標(recall) 經過測試 跟之前訓練的資料差不多\n",
    "\n",
    "可以藉由訓練挑選出適合的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
